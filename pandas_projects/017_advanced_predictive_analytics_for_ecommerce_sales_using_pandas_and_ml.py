import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler


# Data Cleaning and Preprocessing Function
def preprocess_data(df):
    # Handle missing values by forward fill
    df['Sales'].fillna(method='ffill', inplace=True)

    # Create additional time-based features
    df['Weekday'] = pd.to_datetime(df['Date']).dt.weekday
    df['Month'] = pd.to_datetime(df['Date']).dt.month
    df['Year'] = pd.to_datetime(df['Date']).dt.year
    df['DayofYear'] = pd.to_datetime(df['Date']).dt.dayofyear

    return df


# Feature Engineering Function
def feature_engineering(df):
    df['Lag_Sales'] = df['Sales'].shift(1)
    df['Rolling_Mean_Sales'] = df['Sales'].rolling(window=7).mean()
    df['Month_Over_Month'] = df['Sales'] / df['Sales'].shift(30)
    df.dropna(inplace=True)  # Drop NA values generated by shift operations
    return df


# Model Building Function
def train_model(X_train, y_train, model_type='linear'):
    model: LinearRegression | RandomForestRegressor | XGBRegressor
    if model_type == 'linear':
        model = LinearRegression()
        model.fit(X_train, y_train)
        return model
    elif model_type == 'random_forest':
        model = RandomForestRegressor()
        model.fit(X_train, y_train)
        return model
    elif model_type == 'xgboost':
        model = XGBRegressor()
        model.fit(X_train, y_train)
        return model


# Hyperparameter Tuning with Grid Search
def tune_model(X_train, y_train):
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [3, 5],
        'learning_rate': [0.01, 0.1]
    }
    xgb = XGBRegressor()
    grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, verbose=1)
    grid_search.fit(X_train, y_train)
    return grid_search.best_estimator_


# Model Evaluation Function
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    print(f"RMSE: {rmse}")
    print(f"R-Squared: {r2}")

    return y_pred


# Visualizing Predicted vs Actual Sales
def visualize_sales(actual, predicted):
    plt.figure(figsize=(10, 6))
    plt.plot(actual, label='Actual Sales', color='blue')
    plt.plot(predicted, label='Predicted Sales', color='red', linestyle='--')
    plt.title('Sales Prediction vs Actual')
    plt.xlabel('Date')
    plt.ylabel('Sales')
    plt.legend()
    plt.show()


# Main function
if __name__ == "__main__":
    # Simulated input data
    data = {
        'Date': pd.date_range('2024-10-01', periods=30, freq='D'),
        'Sales': np.random.randint(1000, 5000, size=30),
        'Price': np.random.uniform(20, 50, size=30),
        'Quantity Sold': np.random.randint(50, 100, size=30),
        'Marketing Spend': np.random.randint(300, 700, size=30)
    }

    df = pd.DataFrame(data)

    # Preprocess data
    df = preprocess_data(df)

    # Feature engineering
    df = feature_engineering(df)

    # Prepare data for model training
    X = df[['Price', 'Quantity Sold', 'Marketing Spend', 'Lag_Sales', 'Rolling_Mean_Sales', 'Month_Over_Month']]
    y = df['Sales']

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train model
    model = train_model(X_train, y_train, model_type='xgboost')

    # Evaluate model
    y_pred = evaluate_model(model, X_test, y_test)

    # Visualize predictions
    visualize_sales(y_test.values, y_pred)


# https://chatgpt.com/c/6768a17e-2774-800c-a387-7d2cd5bcbaa4 (project 017)
# https://chatgpt.com/c/674b65b9-fecc-800c-8311-7f681df9b305 (all projects)
